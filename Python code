import cv2
import torch
import numpy as np
from pathlib import Path
import supervision as sv
from ultralytics import YOLO
import torch.nn.functional as F
from torchvision import transforms

class SceneUnderstanding:
    def __init__(self):
      
        self.det_model = YOLO('yolov8n.pt')  
        self.seg_model = YOLO('yolov8n-seg.pt')  
        
       
        self.depth_model = torch.hub.load("intel-isl/MiDaS", "MiDaS_small")
        self.depth_model.eval()
        if torch.cuda.is_available():
            self.depth_model.to('cuda')
            
          if matches:
                # Convert all measurements to mm
                w = self.convert_to_mm(float(matches[0][0]), matches[0][1])
                h = self.convert_to_mm(float(matches[0][2]), matches[0][3])
                l = self.convert_to_mm(float(matches[0][4]), matches[0][5])
                return (w, h, l)
            
            return None
            
        except Exception as e:
            print(f"Error searching dimensions for {object_name}: {str(e)}")
            return None
    
    def process_frame(self, frame):
        """Process a single frame for object detection"""
        # Convert frame to PIL Image
        pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
        
        # DETR detection
        inputs = self.processor(images=pil_image, return_tensors="pt")
        outputs = self.model(**inputs)
        
        # Post-process DETR outputs
        target_sizes = torch.tensor([pil_image.size[::-1]])
        results = self.processor.post_process_object_detection(outputs, target_sizes=target_sizes)[0]
        
        detected_objects = []
        
        # Process DETR detections
        for score, label, box in zip(results["scores"], results["labels"], results["boxes"]):
            if score > 0.7:  # Confidence threshold
                box = [round(i) for i in box.tolist()]
                category = self.model.config.id2label[label.item()]
                
                # Update counter for this category
                self.object_counts[category] += 1
                object_name = f"{category} {self.object_counts[category]}"
                
                detected_objects.append({
                    'name': object_name,
                    'box': box,
                    'is_human': category in ['person', 'face']
                })
                
                
              
                moments = cv2.moments(mask.astype(np.uint8))
                if moments["m00"] != 0:
                    cx = int(moments["m10"] / moments["m00"])
                    cy = int(moments["m01"] / moments["m00"])
                    cv2.putText(overlay, class_name, (cx, cy),
                              cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)
        
        return overlay
    

    
    def process_frame(self, frame):
        
        height, width = frame.shape[:2]
        
        
        det_results = self.det_model(frame)[0]
        seg_results = self.seg_model(frame)
        
      
        seg_overlay = self.create_segmentation_overlay(frame, seg_results)
        
        depth_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        depth_frame = torch.from_numpy(depth_frame).float()
        depth_frame = depth_frame.permute(2, 0, 1)
        depth_frame = self.depth_transform(depth_frame).unsqueeze(0)
        
        if torch.cuda.is_available():
            depth_frame = depth_frame.to('cuda')
            
        with torch.no_grad():
            depth_map = self.depth_model(depth_frame)
            depth_map = F.interpolate(depth_map.unsqueeze(1),
                                    size=(height, width),
                                    mode='bilinear',
                                    align_corners=False)
            depth_map = depth_map.squeeze().cpu().numpy()
            
       
        depth_map = (depth_map - depth_map.min()) / (depth_map.max() - depth_map.min())
        depth_map = (depth_map * 255).astype(np.uint8)
        depth_map = cv2.applyColorMap(depth_map, cv2.COLORMAP_PLASMA)
        
       
        results_frame = frame.copy()
        for i, detection in enumerate(det_results.boxes.data.tolist()):
            x1, y1, x2, y2, confidence, class_id = detection
            
           
            x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])
            
            
            class_name = self.det_model.names[int(class_id)]
            
            
            roi_depth = depth_map[y1:y2, x1:x2]
            avg_depth = np.mean(roi_depth)
            relative_dist = f"{avg_depth:.2f}m"
            
          
            color = self.get_color(i)
            cv2.rectangle(results_frame, (x1, y1), (x2, y2), color, 2)
            
            
            label = f"{class_name}: {relative_dist}"
            cv2.putText(results_frame, label, (x1, y1-10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
            
        return results_frame, depth_map, seg_overlay

def main():
    
    cap = cv2.VideoCapture(0)
    scene_understanding = SceneUnderstanding()
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
            
      
        results_frame, depth_map, seg_overlay = scene_understanding.process_frame(frame)
        
    
        segmentation_view = cv2.addWeighted(frame, 0.7, seg_overlay, 0.3, 0)
        
      
        cv2.imshow('Object Detection & Distance', results_frame)
        cv2.imshow('Depth Map', depth_map)
        cv2.imshow('Segmentation', segmentation_view)
        
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
            
    cap.release()
    cv2.destroyAllWindows()

if __name__ == "__main__":
    main()
